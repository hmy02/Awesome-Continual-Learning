# Awesome-Continual-Learning

This is the paper list of our paper ``A Comprehensive Survey of Continual Learning: Theory, Method and Application''.


## Survey

- <a name="todo"></a> [2023 arXiv] Towards Label-Efficient Incremental Learning A Survey [[paper](https://arxiv.org/abs/2302.00353)]

- <a name="todo"></a> [2022 arXiv] Continual Learning of Natural Language Processing Tasks A Survey [[paper](https://arxiv.org/abs/2211.12701)]
  
- <a name="todo"></a> [2022 Trends in Neurosciences] Contributions by metaplasticity to solving the Catastrophic Forgetting Problem [[paper](https://doi.org/10.1016/j.tins.2022.06.002)]

- <a name="todo"></a> [2022 TPAMI] Class-incremental learning survey and performance evaluation on image classification [[paper](https://arxiv.org/abs/2010.15277)]

- <a name='todo'></a> [2022 NMI] Biological underpinnings for lifelong learning machines [[paper](https://www.nature.com/articles/s42256-022-00452-0)]

- <a name='todo'></a> [2022 Neurocomputing] Online Continual Learning in Image Classification_ An Empirical Survey [[paper](https://arxiv.org/abs/2101.10423)]

- <a name='todo'></a> [2022 JAIR] Towards Continual Reinforcement Learning [[paper](https://arxiv.org/abs/2012.13490)]

- <a name='todo'></a> [2021 arXiv] Recent Advances of Continual Learning in Computer Vision_ An Overview [[paper](https://arxiv.org/abs/2109.11369)]

- <a name='todo'></a> [2021 TPAMI] A continual learning survey_ Defying forgetting in classification tasks [[paper](https://arxiv.org/abs/1909.08383)]

- <a name='todo'></a> [2021 Neural Computation] Replay in Deep Learning_ Current Approaches and Missing Biological Elements [[paper](https://arxiv.org/abs/2104.04132)]

- <a name='todo'></a> [2020 Trends in Cognitive Sciences] Embracing Change_ Continual Learning in Deep Neural Networks [[paper](https://www.sciencedirect.com/science/article/pii/S1364661320302199)]

- <a name='todo'></a> [2020 TPAMI] Class-incremental learning survey and performance evaluation on image classification [[paper](https://arxiv.org/abs/2010.15277)]

- <a name='todo'></a> [2020 COLING] Continual Lifelong Learning in Natural Language Processing_ A Survey [[paper](https://arxiv.org/abs/2012.09823)]
  
- <a name="todo"></a> [2019 Neural Networks] Continual Lifelong Learning with Neural Networks: A Review [[paper](https://arxiv.org/abs/1802.07569)]


## Papers

### 2023

- <a name='todo'></a> [2023 ICLR] TASK-AWARE INFORMATION ROUTING FROM COMMON REPRESENTATION SPACE IN LIFELONG LEARNING [[paper](https://arxiv.org/abs/2302.11346)]

### 2022

- <a name='todo'></a> [2022 WACV] Online Continual Learning Via Candidates Voting [[paper](https://arxiv.org/abs/2110.08855v1)]

- <a name='todo'></a> [2022 WACV] Knowledge Capture and Replay for Continual Learning [[paper](https://arxiv.org/abs/2012.06789)]

- <a name='todo'></a> [2022 WACV] FeTrIL Feature Translation for Exemplar-Free Class-Incremental Learning [[paper](https://arxiv.org/abs/2211.13131)]

- <a name='todo'></a> [2022 WACV] Dataset Knowledge Transfer for Class-Incremental Learning without Memory [[paper](https://arxiv.org/abs/2110.08421)]

- <a name='todo'></a> [2022 TPAMI] Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation [[paper](https://arxiv.org/abs/2203.14098)]

- <a name='todo'></a> [2022 TPAMI] MgSvF Multi-Grained Slow vs. Fast Framework for Few-Shot Class-Incremental Learning [[paper](https://arxiv.org/abs/2006.15524)]

- <a name='todo'></a> [2022 TPAMI] Few-Shot Class-Incremental Learning by Sampling Multi-Phase Tasks [[paper](https://arxiv.org/abs/2203.17030)]

- <a name='todo'></a> [2022 TPAMI] Class-Incremental Continual Learning into the eXtended DER-verse [[paper](https://arxiv.org/abs/2201.00766)]

- <a name='todo'></a> [2022 TNNLS] Self-Training for Class-Incremental Semantic Segmentation [[paper](https://arxiv.org/abs/2012.03362)]

- <a name='todo'></a> [2022 PRL] Continual Semi-Supervised Learning through Contrastive Interpolation Consistency  [[paper](https://arxiv.org/abs/2108.06552)]

- <a name='todo'></a> [2022 NeurIPS] Task-Free Continual Learning via Online Discrepancy Distance Learning [[paper](https://arxiv.org/abs/2210.06579)]

- <a name='todo'></a> [2022 NeurIPS] SparCL Sparse Continual Learning on the Edge [[paper](https://arxiv.org/abs/2209.09476)]

- <a name='todo'></a> [2022 NeurIPS] S-Prompts Learning with Pre-trained Transformers An Occamâ€™s Razor for Domain Incremental Learning [[paper](https://arxiv.org/abs/2207.12819)]

- <a name='todo'></a> [2022 NeurIPS] Retrospective Adversarial Replay for Continual Learning [[paper](https://openreview.net/forum?id=XEoih0EwCwL)]

- <a name='todo'></a> [2022 NeurIPS] Repeated Augmented Rehearsal A Simple but Strong Baseline for Online Continual Learning [[paper](https://arxiv.org/abs/2209.13917)]

- <a name='todo'></a> [2022 NeurIPS] On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning [[paper](https://arxiv.org/abs/2210.06443)]

- <a name='todo'></a> [2022 NeurIPS] On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting [[paper](https://arxiv.org/abs/2206.00761)]

- <a name='todo'></a> [2022 NeurIPS] Memory Efficient Continual Learning with Transformers [[paper](https://arxiv.org/abs/2203.04640)]

- <a name='todo'></a> [2022 NeurIPS] Margin-Based Few-Shot Class-Incremental Learning with Class-Level Overfitting Mitigation [[paper](https://arxiv.org/abs/2210.04524)]

- <a name='todo'></a> [2022 NeurIPS] LIFELONG NEURAL PREDICTIVE CODING LEARNING CUMULATIVELY ONLINE WITHOUT FORGETTING [[paper](https://arxiv.org/abs/1905.10696)]

- <a name='todo'></a> [2022 NeurIPS] How Well Do Unsupervised Learning Algorithms Model Human Real-time and Life-long Learning [[paper](https://openreview.net/forum?id=c0l2YolqD2T)]

- <a name='todo'></a> [2022 NeurIPS] Few-Shot Continual Active Learning by a Robot [[paper](https://arxiv.org/abs/2210.04137)]

- <a name='todo'></a> [2022 NeurIPS] Exploring Example Influence in Continual Learning [[paper](https://arxiv.org/abs/2209.12241)]

- <a name='todo'></a> [2022 NeurIPS] Disentangling Transfer in Continual Reinforcement Learning [[paper](https://arxiv.org/abs/2209.13900)]

- <a name='todo'></a> [2022 NeurIPS] Continual Learning In Environments With Polynomial Mixing Times [[paper](https://arxiv.org/abs/2112.07066)]

- <a name='todo'></a> [2022 NeurIPS] Continual learning a feature extraction formalization, an efficient algorithm, and fundamental obstructions [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b63a24a1832bd14fa945c71f535c0095-Abstract-Conference.html)]

- <a name='todo'></a> [2022 NeurIPS] CLiMB A Continual Learning Benchmark for Vision-and-Language Tasks [[paper](https://arxiv.org/abs/2206.09059)]

- <a name='todo'></a> [2022 NeurIPS] CGLB Benchmark Tasks for Continual Graph Learning [[paper](https://papers.nips.cc/paper_files/paper/2022/hash/548a41b9cac6f50dccf7e63e9e1b1b9b-Abstract-Datasets_and_Benchmarks.html)]

- <a name='todo'></a> [2022 NeurIPS] Beyond Not-Forgetting Continual Learning with Backward Knowledge Transfer [[paper](https://arxiv.org/abs/2211.00789)]

- <a name='todo'></a> [2022 NeurIPS] ALIFE Adaptive Logit Regularizer and Feature Replay for Incremental Semantic Segmentation [[paper](https://arxiv.org/abs/2210.06816)]

- <a name='todo'></a> [2022 NeurIPS] A Theoretical Study on Solving Continual Learning [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/20f44da80080d76bbc35bca0027f14e6-Abstract-Conference.html)]

- <a name='todo'></a> [2022 NeurIPSW] A Simple Baseline that Questions the Use of Pretrained-Models in Continual Learning [[paper](https://neurips.cc/virtual/2022/60478)]

- <a name='todo'></a> [2022 Neural Networks] Efficient Perturbation Inference and Expandable Network for Continual Learning [[paper](None)]

- <a name='todo'></a> [2022 NAACL] Overcoming Catastrophic Forgetting During Domain Adaptation of Seq2seq Language Generation [[paper](https://aclanthology.org/2022.naacl-main.398.pdf)]

- <a name='todo'></a> [2022 MM] Semantics-Driven Generative Replay for Few-Shot Class Incremental Learning [[paper](None)]

- <a name='todo'></a> [2022 MM] Incremental Few-Shot Semantic Segmentation via Embedding Adaptive-Update and Hyper-class Representation [[paper](https://github.com/wuyirui)]

- <a name='todo'></a> [2022 MM] Class Gradient Projection For Continual Learning [[paper](https://dl.acm.org/doi/10.1145/3503161.3548054)]

- <a name='todo'></a> [2022 IJCAI] Learning from Students_ Online Contrastive Distillation Network for General Continual Learning [[paper](https://www.ijcai.org/proceedings/2022/446)]

- <a name='todo'></a> [2022 IJCAI] DyGRAIN_ An Incremental Learning Framework for Dynamic Graphs [[paper](https://www.ijcai.org/proceedings/2022/438)]

- <a name='todo'></a> [2022 IJCAI] Continual Semantic Segmentation Leveraging Image-level Labels and Rehearsal [[paper](https://www.ijcai.org/proceedings/2022/177)]

- <a name='todo'></a> [2022 IJCAI] Continual Federated Learning Based on Knowledge Distillation [[paper](https://www.ijcai.org/proceedings/2022/303)]

- <a name='todo'></a> [2022 IJCAI] CERT_ Continual Pre-Training on Sketches for Library-Oriented Code Generation [[paper](https://www.ijcai.org/proceedings/2022/329)]

- <a name='todo'></a> [2022 ICPR] Effects of Auxiliary Knowledge on Continual Learning [[paper](https://arxiv.org/abs/2206.02577v1)]

- <a name='todo'></a> [2022 ICML] Wide Neural Networks Forget Less Catastrophically [[paper](https://arxiv.org/abs/2110.11526)]



### 2021

### 2020

### 2019

### 2018

### 2017

### 2016

